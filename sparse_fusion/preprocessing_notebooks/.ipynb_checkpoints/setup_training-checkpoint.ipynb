{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data For Training\n",
    "This notebook prepares the datasets created/preprocessed for training with `PointNet2`. **NOTE**: It is advised to have already preprocessed your data using the utility functions in `audi_dataset_pre_processing.ipynb` and `audi_dataset_pre_processing.py`. Please note that you may have to modify paths in the functions and scripts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import json\n",
    "import pickle\n",
    "import pptk\n",
    "from pyntcloud import PyntCloud\n",
    "import copy\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2D2 DataLoader\n",
    "This is the same DataLoader as found in the `data_utils/A2D2DataLoader.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2D2DataLoader(Dataset):\n",
    "    def __init__(self, dataset, rotation=None, \\\n",
    "                 normalize_xyz=True, normalize_rgb=True, \\\n",
    "                 take_subset=False, convert_to_tensor=True, \\\n",
    "                 target_ids=[3]):\n",
    "                 #target_ids=[19, 54, 26, 46, 7, 18, 31, 24, 16, 53, 33,\\\n",
    "                             #37, 15, 13, 41, 3, 10, 22, 6, 42, 23, 14, \\\n",
    "                            # 5, 9, 12, 21, 48]):\n",
    "        # Get IDS\n",
    "        self.ids = list(dataset.keys())\n",
    "        \n",
    "        # Get rotation and length of dataset\n",
    "        self.rotation = rotation\n",
    "        self.N = len(self.ids)\n",
    "        \n",
    "        # Get geometric point cloud data and normalize\n",
    "        self.xyz = [dataset[ID]['points'] for ID in self.ids]\n",
    "        self.xyz_norm = self.normalize_xyz()\n",
    "        \n",
    "        # Get rgb data and normalize \n",
    "        self.rgb = [dataset[ID]['rgb'] for ID in self.ids]\n",
    "        self.rgb_norm = self.normalize_rgb()\n",
    "        \n",
    "        # Combine xyz and rgb\n",
    "        self.xyz_rgb = np.hstack((self.xyz, self.rgb))\n",
    "        self.xyz_rgb_norm = [np.hstack((self.xyz_norm[i], self.rgb_norm[i])) for i in range(self.N)]\n",
    "        \n",
    "        # Get labels\n",
    "        self.labels = [dataset[ID]['labels'] for ID in self.ids]\n",
    "        \n",
    "        # Get number of points to use\n",
    "        self.num_points  = np.min([len(self.xyz[i]) for i in range(self.N)])\n",
    "        print(\"SMALLEST PC POINTS: {}\".format(self.num_points))\n",
    "    \n",
    "        if take_subset:\n",
    "            self.target_ids = target_ids\n",
    "            # Now get subset\n",
    "            self.general_dataset, self.target_dataset = self.split_ds_by_classes()\n",
    "        if convert_to_tensor:\n",
    "            self.xyz_norm_tensor, self.rgb_norm_tensor, \\\n",
    "            self.xyz_rgb_norm_tensor, self.labels_tensor = self.convert_to_tensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def rotate_point_cloud_by_angle(self, data, rotation_angle):\n",
    "        \"\"\"\n",
    "        Rotate the point cloud along up direction with certain angle.\n",
    "        :param batch_data: Nx3 array, original batch of point clouds\n",
    "        :param rotation_angle: range of rotation\n",
    "        :return:  Nx3 array, rotated batch of point clouds\n",
    "        \"\"\"\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        rotated_data = np.dot(data, rotation_matrix)\n",
    "\n",
    "        return rotated_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.rotation is not None:\n",
    "            index_xyz = self.xyz[index]\n",
    "            angle = np.random.randint(self.rotation[0], self.rotation[1]) * np.pi / 180\n",
    "            pointcloud = self.rotate_point_cloud_by_angle(index_xyz, angle)\n",
    "\n",
    "            return pointcloud, self.labels[index]\n",
    "        else:\n",
    "            return self.xyz_rgb_norm_tensor[index], self.labels_tensor[index], len(self.xyz_rgb_norm_tensor[index])\n",
    "    \n",
    "    def normalize_xyz(self):\n",
    "        normalized_xyz = []\n",
    "        for ID in range(len(self.ids)):\n",
    "            XYZ = np.copy(self.xyz[ID])\n",
    "            centroid = np.mean(XYZ, axis=0)\n",
    "            XYZ -= centroid\n",
    "            furthest_distance = np.max(np.sqrt(np.sum(abs(XYZ)**2,axis=-1)))\n",
    "            XYZ /= furthest_distance\n",
    "            normalized_xyz.append(XYZ) \n",
    "        print(\"XYZ normalized\")\n",
    "        return normalized_xyz\n",
    "    \n",
    "    def normalize_rgb(self):\n",
    "        normalized_rgb = []\n",
    "        for ID in range(len(self.ids)):\n",
    "            RGB = np.copy(self.rgb[ID])\n",
    "            RGB = np.divide(RGB, 255.0)\n",
    "            normalized_rgb.append(RGB)\n",
    "        print(\"RGB normalized\")\n",
    "        return normalized_rgb\n",
    "    \n",
    "    def convert_to_tensor(self):\n",
    "        \"\"\"\n",
    "        xyz_norm_tensor = torch.tensor(self.xyz_norm)\n",
    "        rgb_norm_tensor = torch.tensor(self.rgb_norm)\n",
    "        xyz_rgb_norm_tensor = torch.tensor(self.xyz_rgb_norm)\n",
    "        labels_tensor = torch.tensor(self.labels)\n",
    "        \"\"\"\n",
    "        xyz_norm_tensor = [torch.tensor(dp) for dp in self.xyz_norm]\n",
    "        rgb_norm_tensor = [torch.tensor(dp) for dp in self.rgb_norm]\n",
    "        xyz_rgb_norm_tensor = [torch.tensor(dp) for dp in self.xyz_rgb_norm]\n",
    "        labels_tensor = [torch.tensor(dp) for dp in self.labels]\n",
    "        \n",
    "        return xyz_norm_tensor, rgb_norm_tensor, xyz_rgb_norm_tensor, labels_tensor\n",
    "    \n",
    "    def split_ds_by_classes(self):\n",
    "        # Init output data structures\n",
    "        gen_ds, target_ds = {}, {}\n",
    "        \n",
    "        general_id_indices = [j for j in range(55) if j not in self.target_ids]\n",
    "        general_id_map = {general_id_indices[k]:k for k in range(len(general_id_indices))}\n",
    "        \n",
    "        # Now make subset and general ID maps and pickle them\n",
    "        self.target_ids.sort()\n",
    "        target_id_map = {self.target_ids[i]:i for i in range(len(self.target_ids))}\n",
    "        target_id_map.update(general_id_map)\n",
    "        \n",
    "        print(\"Target ID Map: \\n {}\".format(target_id_map))\n",
    "        print(\"General ID Map: \\n {}\".format(general_id_map))\n",
    "\n",
    "        # Now pickle these\n",
    "        f_out = os.path.join(os.getcwd(), \"data\", \"ID_MAPS.pkl\")\n",
    "        with open(f_out, \"wb\") as f:\n",
    "            pickle.dump([general_id_indices, general_id_map], f)\n",
    "            f.close()\n",
    "            \n",
    "        for index in range(self.N): # Iterate over all images\n",
    "            FOUND = False\n",
    "            if index % 10000 == 0:\n",
    "                print(\"Iterated through {} files\".format(index))\n",
    "            unique_ids = np.unique(self.labels[index])\n",
    "            for ID in unique_ids:\n",
    "                if ID in self.target_ids:\n",
    "                    labels = self.labels[index]\n",
    "                    mapped_labels = [target_id_map[labels[j]] for j in range(len(labels))]\n",
    "                    target_ds[self.ids[index]] = {'points':self.xyz[index], 'labels':mapped_labels, 'rgb':self.rgb[index]}\n",
    "                    FOUND = True\n",
    "            if not FOUND:\n",
    "                gen_ds[self.ids[index]] = {'points':self.xyz[index], 'labels':self.labels[index], 'rgb':self.rgb[index]}\n",
    "        print(\"Number of pcs in general: {}, Number of pcs in target: {}\".format(len(list(gen_ds.keys())),\\\n",
    "                                                                                 len(list(target_ds.keys()))))\n",
    "        return gen_ds, target_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_wrapper(f_dataset, normalize_xyz=True, normalize_rgb=True,\\\n",
    "                              take_subset=False, convert_to_tensor=True):\n",
    "    # Get input dataset\n",
    "    with open(f_dataset, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    # Instantiate the class object\n",
    "    dataloader = A2D2DataLoader(dataset, normalize_xyz=normalize_xyz, normalize_rgb=normalize_rgb, \\\n",
    "                                take_subset=take_subset, convert_to_tensor=convert_to_tensor)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Full Dataset with All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and save them\n",
    "\n",
    "# Get input dataset\n",
    "f_in = os.path.join(os.getcwd(), \"data\", \"dataset_pc_labels_camera_start_0_stop_10000.pkl\")\n",
    "dataset = create_dataloader_wrapper(f_in, take_subset=False)\n",
    "print(\"Finished processing dataset\")\n",
    "\n",
    "# Create output fname\n",
    "f_out = os.path.join(os.getcwd(),\"data\",\"PROCESSED_mini_dataset_norm_tensor.pkl\")\n",
    "\n",
    "'''\n",
    "# Pickle results\n",
    "with open(f_out, \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "    f.close()\n",
    "''' \n",
    "print(\"Pickled processed dataset to {}\".format(f_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset with General and Target Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input dataset\n",
    "f_in = os.path.join(os.getcwd(), \"data\", \"dataset_pc_labels_camera_start_0_stop_10000_COMBINED_CLASSES.pkl\")\n",
    "dataset = create_dataloader_wrapper(f_in, take_subset=True)\n",
    "print(\"Finished processing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get general and target datasets\n",
    "gen_ds, target_ds = dataset.general_dataset, dataset.target_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create A2D2Dataloader Objects based off of these datasets\n",
    "gen_dataset = A2D2DataLoader(gen_ds, normalize_xyz=True, normalize_rgb=True, \\\n",
    "                                take_subset=False, convert_to_tensor=True)\n",
    "\n",
    "print(\"UNIQUE LABELS: {}\".format(np.unique(gen_dataset.labels[0])))\n",
    "      \n",
    "# Create output fname\n",
    "f_out_general = os.path.join(os.getcwd(),\"data\",\"PROCESSED_general_dataset_start_0_stop_10000_COMBINED CLASSES.pkl\")\n",
    "\n",
    "\n",
    "# Pickle results - general\n",
    "with open(f_out_general, \"wb\") as f:\n",
    "    pickle.dump(gen_ds, f)\n",
    "    f.close()\n",
    "\n",
    "print(\"Pickled general processed dataset to {}\".format(f_out_general))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = A2D2DataLoader(target_ds, normalize_xyz=True, normalize_rgb=True, \\\n",
    "                                take_subset=False, convert_to_tensor=True)\n",
    "\n",
    "print(\"UNIQUE LABELS: {}\".format(np.unique(target_dataset.labels[0])))\n",
    "\n",
    "# Create output fname\n",
    "f_out_target = os.path.join(os.getcwd(),\"data\",\"PROCESSED_target_dataset_start_0_stop_10000_COMBINED_CLASSES.pkl\")\n",
    "\n",
    "# Pickle results - general\n",
    "with open(f_out_target, \"wb\") as f:\n",
    "    pickle.dump(gen_ds, f)\n",
    "    f.close()\n",
    "\n",
    "print(\"Pickled target processed dataset to {}\".format(f_out_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Histogram of Points over Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of number of training points\n",
    "lengths = []\n",
    "for i in range(dataset.N):\n",
    "    lengths.append(min(len(dataset.xyz[i]), 20000))\n",
    "\n",
    "# Now plot\n",
    "plt.hist(lengths,50)\n",
    "plt.title(\"Histogram of Number of Points in Point Cloud for A2D2 Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Dataset Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_datasets(f_pickle):\n",
    "    with open(f_pickle, \"rb\") as f:\n",
    "        D = pickle.load(f)\n",
    "        f.close()\n",
    "    print(D.xyz_norm_tensor[0].numpy().shape)\n",
    "    print(D.rgb_norm_tensor[0].numpy().shape)\n",
    "    print(D.xyz_rgb_norm_tensor[0].numpy().shape)\n",
    "    print(D.labels_tensor[0].numpy().shape)\n",
    "    \n",
    "    print(\"MAX XYZ\", np.max(D.xyz_norm_tensor[0].numpy()))\n",
    "    print(\"MIN XYZ\", np.min(D.xyz_norm_tensor[0].numpy()))\n",
    "    print(\"MAX RGB\", np.max(D.rgb_norm_tensor[0].numpy()))\n",
    "    print(\"MIN RGB\", np.min(D.rgb_norm_tensor[0].numpy()))\n",
    "    print(\"MAX LABEL\", np.max(D.labels_tensor[0].numpy()))\n",
    "    print(\"MIN LABEL\", np.min(D.labels_tensor[0].numpy()))\n",
    "\n",
    "    print(\"GET MIN NUMBER OF POINTS\")\n",
    "    \n",
    "\n",
    "# Run test\n",
    "test_load_datasets(\n",
    "    os.path.join(os.getcwd(),\"data\",\"PROCESSED_mini_dataset_norm_tensor.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use to find which labels are present in which classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seen_matrix(dataloader):\n",
    "    # Now we want to determine which classes appear the fewest number of times\n",
    "    seen = {i: 0 for i in range(55)}\n",
    "    print(seen)\n",
    "    count = 0\n",
    "    for index in range(dataloader.N): # Iterate over each image\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Iterated through {} point clouds\".format(count))\n",
    "        seen_i = {j: 0 for j in range(55)}\n",
    "        for label in dataloader.labels[index]:\n",
    "            seen_i[label] = 1\n",
    "        copy_seen = copy.deepcopy(seen)\n",
    "        seen = {k:seen_i[k]+copy_seen[k] for k in range(55)}\n",
    "        count += 1\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the rarest classes for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_rarest_classes(dataloader,n=10): \n",
    "    seen = create_seen_matrix(dataloader)\n",
    "    sorted_seen = sorted(seen.items(), key=operator.itemgetter(1))\n",
    "    return sorted_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run call for function above\n",
    "get_n_rarest_classes(dataset, n=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Label Counts Across the Entire Dataset\n",
    "The code block below can be used to extract the label counts for datasets with a different number of semantic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance affects performance - let's fix that!\n",
    "full_dataset_file = os.path.join(os.getcwd(), \"data\", \"dataset_pc_labels_camera_start_0_stop_28652.pkl\")\n",
    "\n",
    "# Also compare for combined classes dataset\n",
    "combined_classes_dataset = os.path.join(os.getcwd(), \"data\", \\\n",
    "                            \"dataset_pc_labels_camera_start_0_stop_10000_COMBINED_CLASSES.pkl\")\n",
    "\n",
    "# Also compare for road detection dataset\n",
    "road_detection_dataset = os.path.join(os.getcwd(), \"data\", \\\n",
    "                            \"dataset_pc_labels_camera_start_0_stop_10000_ROAD_DETECTION.pkl\")\n",
    "\n",
    "# Import pickle file\n",
    "with open(road_detection_dataset, \"rb\") as f:\n",
    "    D = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "full_label_counts = {i:0 for i in range(55)}\n",
    "combined_label_counts = {i:0 for i in range(6)}\n",
    "road_detection_label_counts = {i:0 for i in range(2)}\n",
    "    \n",
    "keys = list(D.keys())\n",
    "count = 0\n",
    "\n",
    "for key in keys:\n",
    "    if count % 1000 == 0:\n",
    "        print(\"Processed {} files\".format(count))\n",
    "    for label in D[key]['labels']:\n",
    "        road_detection_label_counts[label] += 1\n",
    "    count += 1\n",
    "\n",
    "print(road_detection_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Class Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle this dictionary to file\n",
    "class_weights_fpath = os.path.join(os.getcwd(), \"data\", \"class_weights_ROAD_DETECTION.pkl\")\n",
    "\n",
    "print(road_detection_label_counts)\n",
    "\n",
    "with open(class_weights_fpath, \"wb\") as f:\n",
    "    pickle.dump(road_detection_label_counts, f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
